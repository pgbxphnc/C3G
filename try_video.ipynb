{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-15T12:11:24.535248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO"
   ],
   "id": "360266667c391956",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mcv2\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01multralytics\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m YOLO\n",
      "File \u001B[1;32mD:\\ana\\envs\\pytorchpy39\\lib\\site-packages\\ultralytics\\__init__.py:11\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39menviron\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOMP_NUM_THREADS\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m      9\u001B[0m     os\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOMP_NUM_THREADS\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1\u001B[39m\u001B[38;5;124m\"\u001B[39m  \u001B[38;5;66;03m# default for reduced CPU utilization during training\u001B[39;00m\n\u001B[1;32m---> 11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01multralytics\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m NAS, RTDETR, SAM, YOLO, FastSAM, YOLOWorld\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01multralytics\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ASSETS, SETTINGS\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01multralytics\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchecks\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m check_yolo \u001B[38;5;28;01mas\u001B[39;00m checks\n",
      "File \u001B[1;32mD:\\ana\\envs\\pytorchpy39\\lib\\site-packages\\ultralytics\\models\\__init__.py:3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Ultralytics üöÄ AGPL-3.0 License - https://ultralytics.com/license\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfastsam\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m FastSAM\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m NAS\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrtdetr\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m RTDETR\n",
      "File \u001B[1;32mD:\\ana\\envs\\pytorchpy39\\lib\\site-packages\\ultralytics\\models\\fastsam\\__init__.py:3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Ultralytics üöÄ AGPL-3.0 License - https://ultralytics.com/license\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m FastSAM\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpredict\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m FastSAMPredictor\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mval\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m FastSAMValidator\n",
      "File \u001B[1;32mD:\\ana\\envs\\pytorchpy39\\lib\\site-packages\\ultralytics\\models\\fastsam\\model.py:5\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Ultralytics üöÄ AGPL-3.0 License - https://ultralytics.com/license\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpathlib\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Path\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01multralytics\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Model\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpredict\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m FastSAMPredictor\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mval\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m FastSAMValidator\n",
      "File \u001B[1;32mD:\\ana\\envs\\pytorchpy39\\lib\\site-packages\\ultralytics\\engine\\model.py:8\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtyping\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Any, Dict, List, Union\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mPIL\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Image\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01multralytics\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcfg\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m TASK2DATA, get_cfg, get_save_dir\n",
      "File \u001B[1;32mD:\\ana\\envs\\pytorchpy39\\lib\\site-packages\\torch\\__init__.py:117\u001B[0m\n\u001B[0;32m    115\u001B[0m is_loaded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m with_load_library_flags:\n\u001B[1;32m--> 117\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mkernel32\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLoadLibraryExW\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdll\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0x00001100\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    118\u001B[0m     last_error \u001B[38;5;241m=\u001B[39m ctypes\u001B[38;5;241m.\u001B[39mget_last_error()\n\u001B[0;32m    119\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m res \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m last_error \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m126\u001B[39m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T08:12:33.445890Z",
     "start_time": "2025-03-15T08:12:33.250917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Âä†ËΩΩÊ®°ÂûãÔºà‰ΩøÁî®YOLOv8xÂ§ßÊ®°ÂûãÔºåÊ£ÄÊµãÁ≤æÂ∫¶Êõ¥È´òÔºâ\n",
    "model = YOLO(\"yolov8x.pt\")\n",
    "\n",
    "# ËæìÂÖ•ËæìÂá∫Ë∑ØÂæÑÈÖçÁΩÆ\n",
    "input_video = \"v_test03.mp4\"          # ËæìÂÖ•ËßÜÈ¢ëË∑ØÂæÑ\n",
    "output_video = \"v_test03_result.mp4\"  # ËæìÂá∫ËßÜÈ¢ëË∑ØÂæÑ"
   ],
   "id": "3f9c2290552fe7d5",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T08:12:46.429487Z",
     "start_time": "2025-03-15T08:12:46.336859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ÊâìÂºÄËßÜÈ¢ëÊñá‰ª∂\n",
    "cap = cv2.VideoCapture(input_video)\n",
    "if not cap.isOpened():\n",
    "    raise FileNotFoundError(f\"ÈîôËØØÔºöÊó†Ê≥ïÊâìÂºÄËßÜÈ¢ëÊñá‰ª∂ {input_video}\")"
   ],
   "id": "578aa2653560c2e3",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T08:12:48.681Z",
     "start_time": "2025-03-15T08:12:48.662064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Ëé∑ÂèñËßÜÈ¢ëÂèÇÊï∞ÔºàÁî®‰∫éËæìÂá∫Êñá‰ª∂ÈÖçÁΩÆÔºâ\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))"
   ],
   "id": "4dd5107b0dfb70ed",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T08:12:50.000123Z",
     "start_time": "2025-03-15T08:12:49.986797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ÂàõÂª∫ËßÜÈ¢ëÂÜôÂÖ•Âô®ÔºàÁ°Æ‰øùÂàÜËæ®Áéá‰∏ÄËá¥Ôºâ\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # ‰ΩøÁî®MP4ÁºñÁ†Å\n",
    "out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))"
   ],
   "id": "8d8e30cadf0f4b0b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T08:12:56.713436Z",
     "start_time": "2025-03-15T08:12:51.867144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ËßÜÈ¢ëÂ§ÑÁêÜÂæ™ÁéØ\n",
    "try:\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        # YOLOv8 ÁõÆÊ†áÊ£ÄÊµã\n",
    "        results = model(frame)  # ‰ΩøÁî®Â§ßÊ®°ÂûãÊé®ÁêÜ\n",
    "\n",
    "        # ÁªòÂà∂Ê£ÄÊµãÁªìÊûú\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        # ÊòæÁ§∫ÂÆûÊó∂ÁîªÈù¢\n",
    "        cv2.imshow(\"YOLOv8 Detection\", annotated_frame)\n",
    "\n",
    "        # ÂÜôÂÖ•ËæìÂá∫ËßÜÈ¢ë\n",
    "        out.write(annotated_frame)\n",
    "\n",
    "        # ÊåâESCÈÄÄÂá∫\n",
    "        if cv2.waitKey(1) == 27:\n",
    "            break\n",
    "finally:\n",
    "    # ÈáäÊîæËµÑÊ∫ê\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"ËßÜÈ¢ëÂàÜÊûêÂÆåÊàêÔºÅÁªìÊûú‰øùÂ≠òËá≥: {output_video}\")"
   ],
   "id": "d7ec8eb651717f2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 416x640 7 persons, 47.5ms\n",
      "Speed: 4.2ms preprocess, 47.5ms inference, 98.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 7 persons, 1 baseball glove, 27.4ms\n",
      "Speed: 2.2ms preprocess, 27.4ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 7 persons, 1 baseball glove, 27.2ms\n",
      "Speed: 2.8ms preprocess, 27.2ms inference, 2.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 7 persons, 2 baseball gloves, 27.3ms\n",
      "Speed: 3.2ms preprocess, 27.3ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 7 persons, 26.3ms\n",
      "Speed: 1.7ms preprocess, 26.3ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 7 persons, 1 baseball glove, 1 tennis racket, 25.4ms\n",
      "Speed: 1.5ms preprocess, 25.4ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 7 persons, 1 tennis racket, 25.3ms\n",
      "Speed: 1.4ms preprocess, 25.3ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 7 persons, 1 tennis racket, 25.3ms\n",
      "Speed: 1.4ms preprocess, 25.3ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 8 persons, 1 tennis racket, 25.3ms\n",
      "Speed: 1.5ms preprocess, 25.3ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 8 persons, 1 tennis racket, 23.5ms\n",
      "Speed: 1.4ms preprocess, 23.5ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 8 persons, 1 tennis racket, 23.4ms\n",
      "Speed: 1.5ms preprocess, 23.4ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 8 persons, 23.4ms\n",
      "Speed: 1.5ms preprocess, 23.4ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 8 persons, 23.4ms\n",
      "Speed: 1.4ms preprocess, 23.4ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 9 persons, 23.4ms\n",
      "Speed: 1.5ms preprocess, 23.4ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 8 persons, 23.4ms\n",
      "Speed: 1.6ms preprocess, 23.4ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 7 persons, 23.4ms\n",
      "Speed: 1.7ms preprocess, 23.4ms inference, 2.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 8 persons, 23.4ms\n",
      "Speed: 1.4ms preprocess, 23.4ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 8 persons, 23.4ms\n",
      "Speed: 1.4ms preprocess, 23.4ms inference, 2.3ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 8 persons, 23.6ms\n",
      "Speed: 1.4ms preprocess, 23.6ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 8 persons, 23.6ms\n",
      "Speed: 1.4ms preprocess, 23.6ms inference, 2.3ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 7 persons, 23.5ms\n",
      "Speed: 1.4ms preprocess, 23.5ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 6 persons, 23.4ms\n",
      "Speed: 1.5ms preprocess, 23.4ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 7 persons, 23.5ms\n",
      "Speed: 1.9ms preprocess, 23.5ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 6 persons, 23.5ms\n",
      "Speed: 2.6ms preprocess, 23.5ms inference, 2.7ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 6 persons, 23.4ms\n",
      "Speed: 2.1ms preprocess, 23.4ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 6 persons, 23.7ms\n",
      "Speed: 1.4ms preprocess, 23.7ms inference, 2.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 5 persons, 1 sports ball, 1 tennis racket, 23.7ms\n",
      "Speed: 1.7ms preprocess, 23.7ms inference, 2.3ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 4 persons, 1 sports ball, 23.5ms\n",
      "Speed: 1.7ms preprocess, 23.5ms inference, 21.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 3 persons, 1 sports ball, 23.5ms\n",
      "Speed: 1.3ms preprocess, 23.5ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 5 persons, 1 sports ball, 23.4ms\n",
      "Speed: 1.3ms preprocess, 23.4ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 5 persons, 1 sports ball, 23.4ms\n",
      "Speed: 1.5ms preprocess, 23.4ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 5 persons, 1 sports ball, 23.5ms\n",
      "Speed: 1.9ms preprocess, 23.5ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 4 persons, 1 sports ball, 23.4ms\n",
      "Speed: 1.4ms preprocess, 23.4ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 5 persons, 2 sports balls, 1 tennis racket, 23.5ms\n",
      "Speed: 1.5ms preprocess, 23.5ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 5 persons, 1 sports ball, 23.5ms\n",
      "Speed: 1.2ms preprocess, 23.5ms inference, 2.3ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 4 persons, 23.6ms\n",
      "Speed: 1.3ms preprocess, 23.6ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 4 persons, 23.3ms\n",
      "Speed: 1.4ms preprocess, 23.3ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 4 persons, 23.5ms\n",
      "Speed: 1.4ms preprocess, 23.5ms inference, 3.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 4 persons, 23.8ms\n",
      "Speed: 2.3ms preprocess, 23.8ms inference, 2.7ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 4 persons, 23.5ms\n",
      "Speed: 2.0ms preprocess, 23.5ms inference, 2.3ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 4 persons, 23.7ms\n",
      "Speed: 2.4ms preprocess, 23.7ms inference, 2.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 5 persons, 23.5ms\n",
      "Speed: 1.3ms preprocess, 23.5ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 5 persons, 23.3ms\n",
      "Speed: 1.2ms preprocess, 23.3ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 5 persons, 23.5ms\n",
      "Speed: 1.2ms preprocess, 23.5ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 4 persons, 23.4ms\n",
      "Speed: 1.2ms preprocess, 23.4ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 4 persons, 1 tennis racket, 24.7ms\n",
      "Speed: 1.3ms preprocess, 24.7ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 4 persons, 23.4ms\n",
      "Speed: 1.4ms preprocess, 23.4ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 4 persons, 23.5ms\n",
      "Speed: 1.2ms preprocess, 23.5ms inference, 2.3ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 5 persons, 23.4ms\n",
      "Speed: 1.5ms preprocess, 23.4ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 5 persons, 23.4ms\n",
      "Speed: 1.3ms preprocess, 23.4ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 640)\n",
      "ËßÜÈ¢ëÂàÜÊûêÂÆåÊàêÔºÅÁªìÊûú‰øùÂ≠òËá≥: v_test03_result.mp4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 21\u001B[0m\n\u001B[0;32m     18\u001B[0m         out\u001B[38;5;241m.\u001B[39mwrite(annotated_frame)\n\u001B[0;32m     20\u001B[0m         \u001B[38;5;66;03m# ÊåâESCÈÄÄÂá∫\u001B[39;00m\n\u001B[1;32m---> 21\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mcv2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwaitKey\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m27\u001B[39m:\n\u001B[0;32m     22\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;66;03m# ÈáäÊîæËµÑÊ∫ê\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchpy39",
   "language": "python",
   "name": "pytorchpy39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
